{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32214a35",
   "metadata": {},
   "source": [
    "# ques-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f293b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min-Max scaling is a data preprocessing technique used to normalize the range of numerical features in a dataset. It involves scaling the values of a feature to a specific range, usually between 0 and 1. The formula for Min-Max scaling is given by:\n",
    "\n",
    "#Min-Max scaling is particularly useful when the features in a dataset have different scales, and some machine learning algorithms might be sensitive to the scale of the input features. By scaling the features to a common range, Min-Max scaling ensures that each feature contributes equally to the analysis.\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import numpy as np\n",
    "\n",
    "# # Sample data with different scales\n",
    "# data = np.array([[1.0, 2.0, 3.0],\n",
    "#                  [4.0, 5.0, 6.0],\n",
    "#                  [7.0, 8.0, 9.0]])\n",
    "\n",
    "# # Initialize MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# # Fit and transform the data\n",
    "# scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# print(\"Original Data:\")\n",
    "# print(data)\n",
    "\n",
    "# print(\"\\nScaled Data (Min-Max scaled):\")\n",
    "# print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eacc546",
   "metadata": {},
   "source": [
    "# ques-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e25b914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Unit Vector technique, also known as vector normalization, is a feature scaling method that scales each feature in a dataset by dividing each value by the magnitude (or length) of the feature vector. This ensures that each feature has a unit magnitude, transforming the data points into points on the unit hypersphere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16db8520",
   "metadata": {},
   "source": [
    "# ques-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25448ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while retaining the most important information. It achieves this by identifying and capturing the principal components, which are the directions of maximum variance in the data.\n",
    "\n",
    "# The basic idea behind PCA is to find a set of orthogonal axes (principal components) in the feature space along which the data varies the most. These components are ranked by the amount of variance they capture, allowing for dimensionality reduction while preserving the essential characteristics of the data.\n",
    "# Dimensionality Reduction:\n",
    "\n",
    "# PCA is primarily used for reducing the number of features in a dataset while retaining the most critical information. This is valuable when dealing with high-dimensional data, such as images, genetic data, or financial datasets.\n",
    "# Data Visualization:\n",
    "\n",
    "# PCA can be employed for visualizing high-dimensional data in a lower-dimensional space. By representing data points in the space defined by the principal components, patterns and relationships between data points become more apparent.\n",
    "# Noise Reduction:\n",
    "\n",
    "# When datasets have redundant or irrelevant features, PCA helps in removing such noise by capturing the essential features that contribute most to the variance in the data.\n",
    "# Feature Extraction:\n",
    "\n",
    "# PCA is used to transform a set of possibly correlated features into a new set of linearly uncorrelated features (principal components). These principal components can be considered as new features that often represent combinations of the original features.\n",
    "# Image Compression:\n",
    "\n",
    "# In image processing, PCA can be used for compressing images by reducing the dimensionality of the pixel values. This is particularly useful for applications where storage or transmission bandwidth is limited.\n",
    "# Face Recognition:\n",
    "\n",
    "# PCA has been applied to facial recognition systems. By representing faces in a lower-dimensional space, it becomes easier to compare and recognize faces, especially in scenarios with varying lighting conditions or facial expressions.\n",
    "# Biological Data Analysis:\n",
    "\n",
    "# In genomics and bioinformatics, PCA is used to analyze gene expression data. It helps identify patterns and relationships between genes, facilitating the discovery of biologically significant information.\n",
    "# Finance and Economics:\n",
    "\n",
    "# In finance, PCA is applied to analyze and model the covariance structure of asset returns. It is used for risk management, portfolio optimization, and identifying factors that contribute to the overall variability in financial markets.\n",
    "# Spectral Analysis:\n",
    "\n",
    "# In signal processing and remote sensing, PCA is used for spectral analysis of data. It helps extract relevant information from signals with multiple spectral bands.\n",
    "# Machine Learning Preprocessing:\n",
    "\n",
    "# PCA is often used as a preprocessing step in machine learning pipelines to reduce the dimensionality of the input features and improve the performance of machine learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a442a5",
   "metadata": {},
   "source": [
    "# ques-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6da5e692",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Principal Component Analysis (PCA) is closely related to feature extraction, as one of its primary applications is extracting relevant features from a high-dimensional dataset. In the context of PCA, feature extraction refers to transforming the original features into a new set of features, called principal components, that capture the most significant information in the data.\n",
    "\n",
    "# The relationship between PCA and feature extraction can be summarized as follows:\n",
    "\n",
    "# Linear Combination of Features:\n",
    "\n",
    "# PCA represents each principal component as a linear combination of the original features. The weights (coefficients) assigned to each original feature in these linear combinations are determined by the eigenvectors of the covariance matrix.\n",
    "# Variance Capture:\n",
    "\n",
    "# Principal components are ordered by the amount of variance they capture in the data. The first few principal components typically capture the majority of the variance, making them a condensed representation of the original features.\n",
    "# Dimensionality Reduction:\n",
    "\n",
    "# The main goal of PCA is to reduce the dimensionality of the dataset while retaining as much variance as possible. This reduction is achieved by selecting a subset of the principal components that collectively explain a significant portion of the total variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4e9b4c",
   "metadata": {},
   "source": [
    "# ques-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc213f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Given dataset\n",
    "# dataset = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# # Min-Max scaling parameters\n",
    "# new_min = -1\n",
    "# new_max = 1\n",
    "\n",
    "# # Calculate Min-Max scaling\n",
    "# scaled_values = ((dataset - np.min(dataset)) / (np.max(dataset) - np.min(dataset))) * (new_max - new_min) + new_min\n",
    "\n",
    "# print(\"Original Dataset:\")\n",
    "# print(dataset)\n",
    "\n",
    "# print(\"\\nMin-Max Scaled Dataset (in the range of -1 to 1):\")\n",
    "# print(scaled_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
